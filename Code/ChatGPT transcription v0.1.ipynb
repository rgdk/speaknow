{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a225d6f2-bb49-40d3-bf27-7856d11990df",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfa6acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ =load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d913598-7af1-4605-9d0f-7fa4e99332a2",
   "metadata": {},
   "source": [
    "#### Script Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3a986d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/danie/Speaknow/projects\")\n",
    "\n",
    "# Specify the path to the directory containing the MP3 files\n",
    "audio_folder_dir = './Data/audio'\n",
    "audio_training_dir = \"./Data/training/audio\"\n",
    "transcript_output_dir = \"./Data/training/transcripts\"\n",
    "chatgpt_transcripts = \"./Data/training/chatgpt_transcripts\"\n",
    "\n",
    "# List all files in the audio directory\n",
    "audio_files = os.listdir(audio_folder_dir)\n",
    "mp3_files = [file for file in audio_files if file.endswith('.mp3')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0aede1-4163-4cf3-9f81-c29e590d78ad",
   "metadata": {},
   "source": [
    "#### Executing the ChatGPT Whisper-1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9429deeb-392a-4859-a602-7184f36699da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcription job completed\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "\n",
    "for mp3_file in mp3_files:\n",
    "    mp3_file_path = audio_folder_dir + \"/\" + mp3_file  \n",
    "\n",
    "    audio_file = open(mp3_file_path, \"rb\")\n",
    "\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      response_format=\"text\",\n",
    "      language='en',\n",
    "      temperature=0.2\n",
    "    )\n",
    "    text_file_path = os.path.join(chatgpt_transcripts, os.path.splitext(mp3_file)[0] + \".txt\")\n",
    "    with open(text_file_path, \"w\") as text_file:\n",
    "        text_file.write(transcript)\n",
    "    text_file.close()\n",
    "\n",
    "print(\"transcription job completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d6514-51b5-4787-b64f-ee120318d141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
