{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec5a2a9-c7d7-4d3b-b6f7-375e67848b64",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881c3ed1-01b7-434b-a1d5-4c8f1e5ef671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3f36a-f181-4e29-85d8-110cb20fadac",
   "metadata": {},
   "source": [
    "##### Configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031b29f6-07e6-45a0-b077-6b865011cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/danie/Speaknow/projects\")\n",
    "\n",
    "# Specify the path to the directory containing the MP3 files\n",
    "folder_path = './Data/audio'\n",
    "audio_dir = \"./Data/training/audio\"\n",
    "wav_file_directory = \"./Data/training/audio\"\n",
    "chatgpt_transcripts = \"./Data/training/chatgpt_transcripts\"\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Filter out only the MP3 files\n",
    "mp3_files = [file for file in files if file.endswith('.mp3')]\n",
    "# wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Load pre-trained GloVe word embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "    \n",
    "glove_file = \"./Model/pretrained/Glove6B/glove.6B.100d.txt\"  # Path to pre-trained GloVe embeddings file\n",
    "embeddings = load_glove_embeddings(glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a6972-2328-4a71-9a00-2375866ce091",
   "metadata": {},
   "source": [
    "##### Convert mp3 files to wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8765ec-9968-4f71-b415-33db2fa2d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mp3_to_wav(audio_input_dir, audio_output_dir):\n",
    "    # Loop through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if it's an mp3 file\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            # Define input and output filenames\n",
    "            input_file = folder_path + \"/\" + filename\n",
    "            output_file = os.path.splitext(input_file)[0] + \".wav\"\n",
    "    \n",
    "            print(input_file)\n",
    "            print(output_file)\n",
    "            # Load the audio segment\n",
    "            sound = AudioSegment.from_mp3(input_file)\n",
    "    \n",
    "            # Export the audio as wav\n",
    "            sound.export(output_file, format=\"wav\")\n",
    "    \n",
    "            print(f\"Converted {input_file} to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c67f3d-39f5-4b6b-bfc0-3ee890370397",
   "metadata": {},
   "source": [
    "##### Generate transcripts using the HuggingFace tokenizer \n",
    "\n",
    "Note: This model has not been fine-tuned (yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae3423-6e46-40d3-96f1-fccf692cadda",
   "metadata": {},
   "source": [
    "##### Load and preprocess the audio files so that they are normalized and noise-reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57452c29-2d35-4765-af0f-db24f24bac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_file, target_sr=16000, noise_threshold=0.005):\n",
    "    \"\"\"\n",
    "    Preprocess audio file.\n",
    "    \n",
    "    Args:\n",
    "    - audio_file: path to the audio file\n",
    "    - target_sr: target sampling rate (default: 16000 Hz)\n",
    "    - noise_threshold: threshold for noise reduction (default: 0.005)\n",
    "    \n",
    "    Returns:\n",
    "    - audio_data: preprocessed audio data (numpy array)\n",
    "    - sampling_rate: sampling rate of the audio data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load entire audio file\n",
    "        audio_data, sampling_rate = librosa.load(audio_file, sr=target_sr, mono=False, duration=None)\n",
    "\n",
    "        audio_duration = len(audio_data) / sampling_rate\n",
    "\n",
    "        # # Ensure fixed duration\n",
    "        # if len(audio_data) < target_sr * duration:\n",
    "        #     shortage = target_sr * duration - len(audio_data)\n",
    "        #     audio_data = np.pad(audio_data, (0, shortage), mode='constant')\n",
    "\n",
    "        if audio_data.ndim == 1:\n",
    "            channels = 1  # Mono audio\n",
    "        else:\n",
    "            channels = y.shape[0]\n",
    "\n",
    "        # Normalize audio data\n",
    "        audio_data = librosa.util.normalize(audio_data)\n",
    "\n",
    "        # Apply noise reduction\n",
    "        threshold = np.max(audio_data) * noise_threshold\n",
    "        audio_data[np.abs(audio_data) < threshold] = 0\n",
    "\n",
    "        return audio_data, sampling_rate, channels, audio_duration \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file {audio_file}: {e}\")\n",
    "        return None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de81c97-98d6-40cd-9d9a-ad15444c83fb",
   "metadata": {},
   "source": [
    "##### Extract relevant features from the audio files to be used in the models\n",
    "These are features that are derived from the sound file itself and not from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24531a0a-3fe4-4409-8f34-71681b66e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_data, sampling_rate):\n",
    "    \"\"\"\n",
    "    Extract relevant features from the audio data.\n",
    "    \n",
    "    Args:\n",
    "    - audio_data: preprocessed audio data (numpy array)\n",
    "    - sampling_rate: sampling rate of the audio data\n",
    "    \n",
    "    Returns:\n",
    "    features containing:\n",
    "    - formants: Resonant frequencies of the vocal tract\n",
    "        --> flattened into mean, median, std, min and max\n",
    "    - pitch: Fundamental frequency (F0) of the speech signal\n",
    "    - intensity: Energy level of the speech signal\n",
    "    - speech_rate: Number of words spoken per unit time\n",
    "    - pauses_duration: Total duration of pauses in speech (in seconds)\n",
    "    - pauses_frequency: Frequency of pauses in speech (number of pauses per second)\n",
    "    - audio_duration: Duration of the audio signal (in seconds)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract MFCCs (Mel-Frequency Cepstral Coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sampling_rate, n_mfcc=13)\n",
    "        mfccs_mean = np.mean(np.mean(mfccs, axis=1))\n",
    "        mfccs_std = np.std(np.std(mfccs, axis=1))\n",
    "        mfccs_var = np.var(np.var(mfccs, axis=1))\n",
    "        \n",
    "        # Extract spectral centroid\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sampling_rate)\n",
    "        spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "        \n",
    "        # Extract spectral bandwidth\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sampling_rate)\n",
    "        spectral_bandwidth_mean = np.mean(spectral_bandwidth)\n",
    "    \n",
    "        # Extract pitch (fundamental frequency)\n",
    "        pitch, _ = librosa.piptrack(y=audio_data, sr=sampling_rate)\n",
    "        pitch = np.mean(pitch[pitch > 0])  # Average non-zero pitch values\n",
    "    \n",
    "        # Extract intensity (energy level)\n",
    "        intensity = np.mean(librosa.feature.rms(y=audio_data))\n",
    "    \n",
    "        # Extract formants (resonant frequencies)\n",
    "        formants = librosa.effects.harmonic(audio_data)\n",
    "        formants = librosa.feature.spectral_centroid(y=formants, sr=sampling_rate)\n",
    "        \n",
    "        # Flatten formants\n",
    "        formants_flat = [np.mean(formants), np.median(formants), np.std(formants), np.min(formants), np.max(formants)]\n",
    "    \n",
    "        # Extract speech rate (number of words spoken per unit time)\n",
    "        # We can estimate speech rate by counting the number of zero-crossings in the audio signal\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=audio_data)\n",
    "        speech_rate = np.mean(zcr)\n",
    "\n",
    "        intensity_to_speech_rate_ratio = intensity / speech_rate\n",
    "        \n",
    "        # Extract pauses (duration and frequency)\n",
    "        # We can estimate pauses by detecting segments of low energy in the audio signal\n",
    "        energy = librosa.feature.rms(y=audio_data)\n",
    "        threshold = np.mean(energy) * 0.5  # Using 50% of the mean energy as threshold\n",
    "        pauses_duration = np.sum(energy[0] < threshold) / sampling_rate  # Total duration of pauses in seconds\n",
    "        pauses_frequency = np.mean(energy[0] < threshold)  # Frequency of pauses per second\n",
    "    \n",
    "        # Calculate the duration of the audio signal\n",
    "        audio_duration = len(audio_data) / sampling_rate\n",
    "    \n",
    "        features = pd.DataFrame({\"formants_mean\": np.mean(formants), \n",
    "                                 \"formants_median\": np.median(formants), \n",
    "                                 \"formants_std\": np.std(formants), \n",
    "                                 \"formants_min\": np.min(formants), \n",
    "                                 \"formants_max\": np.max(formants), \n",
    "                                 \"mfccs_mean\": mfccs_mean,\n",
    "                                 \"mfccs_std\": mfccs_std, \n",
    "                                 \"mfccs_var\": mfccs_var,\n",
    "                                 \"spectral_centroid_mean\": spectral_centroid_mean, \n",
    "                                 \"spectral_bandwidth_mean\": spectral_bandwidth_mean, \n",
    "                                 \"pitch\": pitch, \n",
    "                                 \"intensity\": intensity, \n",
    "                                 \"speech_rate\": speech_rate, \n",
    "                                 \"intensity_to_speech_rate_ratio\": intensity_to_speech_rate_ratio,\n",
    "                                 \"pauses_duration\": pauses_duration, \n",
    "                                 \"pauses_frequency\": pauses_frequency}, index=[0])\n",
    "    \n",
    "        return features.reset_index(drop=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio features: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1409f92-1be8-4117-99c4-193206f93a5d",
   "metadata": {},
   "source": [
    "##### Run the preprocessing and feature extraction functions for each audio file and return a corresponding feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3676a78-d095-483b-bed8-df283b2f034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_audio_features(mp3_files):\n",
    "    features = []\n",
    "    features_df = pd.DataFrame([])\n",
    "    \n",
    "    for audio_file in mp3_files:\n",
    "        audio_file = folder_path+\"/\"+audio_file\n",
    "        \n",
    "        preprocessed_audio, sampling_rate, channels, audio_duration = preprocess_audio(audio_file)\n",
    "    \n",
    "        _, audio_file = os.path.split(audio_file) #remove the folder path\n",
    "        audio_file, _ = os.path.splitext(audio_file) # remove the file extension\n",
    "        \n",
    "        initial_features = pd.DataFrame({\"audio_file\": audio_file, \"sampling_rate\": sampling_rate, \"channels\": channels, \"audio_duration\": audio_duration}, index=[0]) \n",
    "        \n",
    "        features = extract_audio_features(preprocessed_audio, sampling_rate)\n",
    "    \n",
    "        temp_df = pd.concat([initial_features, features], axis=1)\n",
    "    \n",
    "        features_df = pd.concat([features_df, temp_df], axis=0)\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2149d-b074-4c78-82e3-3c27d462d222",
   "metadata": {},
   "source": [
    "##### Load the csv file into a dataframe & add the transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1362c819-11b3-4142-b30b-027aeb213cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and return the transcript text from file \n",
    "def read_transcript(transcript_dir, file_name, suffix):\n",
    "    try:\n",
    "        fn = os.path.join(transcript_dir, f\"{file_name}-{suffix}.txt\")\n",
    "        with open(fn, 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return None  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff9220-1c8f-4eba-95c3-dad0a5aaac31",
   "metadata": {},
   "source": [
    "##### Reshape the dataframe so that all transcripts from common assesment id's are on separate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e32a63-3d8c-4214-89b6-e1a65f3f44b1",
   "metadata": {},
   "source": [
    "##### Compute the Brunets Index\n",
    "This gives an idea of the lexical diversity of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81d7b44a-fd66-4d49-a870-882bca9e44fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assessment_id</th>\n",
       "      <th>pronunciation_avg</th>\n",
       "      <th>vocab_avg</th>\n",
       "      <th>fluency_avg</th>\n",
       "      <th>cohesion_avg</th>\n",
       "      <th>grammar_avg</th>\n",
       "      <th>cefr_avg</th>\n",
       "      <th>question_1_transcript</th>\n",
       "      <th>question_2_transcript</th>\n",
       "      <th>question_3_transcript</th>\n",
       "      <th>question_4_transcript</th>\n",
       "      <th>question_5_transcript</th>\n",
       "      <th>transcript-1</th>\n",
       "      <th>transcript-2</th>\n",
       "      <th>transcript-3</th>\n",
       "      <th>transcript-4</th>\n",
       "      <th>transcript-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692530001326214</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.5</td>\n",
       "      <td>So my favorite TV show would be the, uh the Fi...</td>\n",
       "      <td>Uh, there are actually many different jobs I d...</td>\n",
       "      <td>Uh, I consider charity organizations important...</td>\n",
       "      <td>I, I think that the home schooling becomes uh ...</td>\n",
       "      <td>I consider studying foreign languages. Uh real...</td>\n",
       "      <td>So my favorite TV show would be The Firefly. I...</td>\n",
       "      <td>There are actually many different jobs I don't...</td>\n",
       "      <td>I consider charity organizations important bec...</td>\n",
       "      <td>I think that homeschooling became really popul...</td>\n",
       "      <td>I consider studying foreign languages really i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1647885061811312</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yeah, I would be happy to go to the past and m...</td>\n",
       "      <td>I prefer to be most remembered for my great wi...</td>\n",
       "      <td>I prefer to be the first person to explore a p...</td>\n",
       "      <td>I think there should be restrictions for publi...</td>\n",
       "      <td>Yeah, I prefer to be remembered for my great w...</td>\n",
       "      <td>I would be happy to go to the past and meet my...</td>\n",
       "      <td>I prefer to be most remembered for my great wi...</td>\n",
       "      <td>I prefer to be the first person to explore a p...</td>\n",
       "      <td>I think there should be restrictions for publi...</td>\n",
       "      <td>Yeah, I prefer to be remembered for my great w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1667330281849843</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I would much rather be a first version me than...</td>\n",
       "      <td>This is an excellent question. I think I would...</td>\n",
       "      <td>This is an interesting question. The potential...</td>\n",
       "      <td>I think that this is an excellent question. It...</td>\n",
       "      <td>As a student for an All girls high school, I a...</td>\n",
       "      <td>I would much rather be a first version me than...</td>\n",
       "      <td>This is an excellent question. I think I would...</td>\n",
       "      <td>This is an interesting question. The potential...</td>\n",
       "      <td>I think that this is an excellent question. It...</td>\n",
       "      <td>As a student for an all-girls high school, I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1691402791754105</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>My advice to a worker in my office is if you d...</td>\n",
       "      <td>Uh, I would, I would love to communicate with ...</td>\n",
       "      <td>That's a good question. I do not have, I don't...</td>\n",
       "      <td>Ok. Then if I'll have an, if I have an ideal p...</td>\n",
       "      <td>In my city. Uh the system is that uh everyone ...</td>\n",
       "      <td>My advice to a worker in my office is if you d...</td>\n",
       "      <td>I would love to communicate with dolphins. I d...</td>\n",
       "      <td>That's a good question. I do not have... I don...</td>\n",
       "      <td>Okay then, if I have an ideal project, I would...</td>\n",
       "      <td>In my city, the system is that everyone should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1691402791754221</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>I'm working in the production, in my, in, in m...</td>\n",
       "      <td>I'm, uh, got to talk in the teacher of the, an...</td>\n",
       "      <td>Er, I like to go to the, er, in engineering, i...</td>\n",
       "      <td>No, I just, uh, stay just me. I, I like the, w...</td>\n",
       "      <td>No, I don't want to move to a new city or, uh,...</td>\n",
       "      <td>I am working in the production in my job, I am...</td>\n",
       "      <td>I am going to talk to the teacher of the Engli...</td>\n",
       "      <td>I like to go to the engineering in my workplac...</td>\n",
       "      <td>No, I just stay just me. I like what I do, I l...</td>\n",
       "      <td>No, I don't want to move to a new city or to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1687964058482484</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>My favorite class in school is informatics. Er...</td>\n",
       "      <td>My family is, uh, make me happy. Um, my family...</td>\n",
       "      <td>In the last, in the vacation of last year I tr...</td>\n",
       "      <td>In my time happy when my sister, not, not the ...</td>\n",
       "      <td>I don't think, er, planning, er, to last summe...</td>\n",
       "      <td>My favorite class in school is informatics bec...</td>\n",
       "      <td>My family is make me happy. My family is more....</td>\n",
       "      <td>In the last, in the vacation of last year, I t...</td>\n",
       "      <td>I'm happy when my sister not destroy the house...</td>\n",
       "      <td>I don't think planning to next summer. Where w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1685651809260972</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>I use, uh, uh, computer because it, it's so fu...</td>\n",
       "      <td>We are six people and my father is uh is a war...</td>\n",
       "      <td>I'm gonna talk about my friend, my best friend...</td>\n",
       "      <td>And for me, uh, an interesting, some, somethin...</td>\n",
       "      <td>To the next summer I will go to, to the don't ...</td>\n",
       "      <td>I use a computer because it's so fun to use it...</td>\n",
       "      <td>We are six people and my father is a work, is ...</td>\n",
       "      <td>I'm gonna talk about my friend, my best friend...</td>\n",
       "      <td>And for me, an interesting, something interest...</td>\n",
       "      <td>to the next summer i will go to the down cost ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1662420268098487</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>the person who is the most important people. N...</td>\n",
       "      <td>my best teacher was maybe, I don't know. My my...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Well, I think um different um work office is n...</td>\n",
       "      <td>is is um, difficult question because, um, I th...</td>\n",
       "      <td>The person who is the most important person in...</td>\n",
       "      <td>My best teacher was maybe, I don't know, my fi...</td>\n",
       "      <td>Thank you. Bye bye. Bye bye. Bye bye. Bye bye....</td>\n",
       "      <td>Well, I think different work office is not usu...</td>\n",
       "      <td>It's a difficult question because I think the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1687976114519479</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I will travel with my parents to Colombia beca...</td>\n",
       "      <td>I think the biggest problem for kids today is ...</td>\n",
       "      <td>I didn't know about the system of recycling in...</td>\n",
       "      <td>If I start my own business, it will be a cafet...</td>\n",
       "      <td>I think high school students will have to wear...</td>\n",
       "      <td>I will travel with my parents to Colombia beca...</td>\n",
       "      <td>I think the biggest problem for kids today is ...</td>\n",
       "      <td>I didn't know about the system of recycling in...</td>\n",
       "      <td>If I start my own business it will be a cafete...</td>\n",
       "      <td>I think high school students should have to we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1687976114519971</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>A life in Panama. Um, um, my house is big and ...</td>\n",
       "      <td>I'm going to talk about, of my mom and my mom ...</td>\n",
       "      <td>And my favorite vacation is when I go to Cancu...</td>\n",
       "      <td>Ok. My, my friend is, um, his wife is a small ...</td>\n",
       "      <td>Ok. In my next summer I will play, er, I love ...</td>\n",
       "      <td>My life in Panama, my house is big and I can d...</td>\n",
       "      <td>I'm going to talk about my mom. My mom is impo...</td>\n",
       "      <td>My favorite vacation is when I go to Cancun be...</td>\n",
       "      <td>Okay, my friend is white, he is a small person...</td>\n",
       "      <td>Okay, my next summer I will play a lot of Play...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       assessment_id  pronunciation_avg  vocab_avg  fluency_avg  cohesion_avg  \\\n",
       "0   1692530001326214               4.50       4.50         4.00           4.0   \n",
       "1   1647885061811312               3.33       4.33         3.67           4.0   \n",
       "2   1667330281849843               5.50       5.00         4.50           5.0   \n",
       "3   1691402791754105               4.00       4.00         3.50           3.0   \n",
       "4   1691402791754221               2.50       3.00         2.50           2.5   \n",
       "..               ...                ...        ...          ...           ...   \n",
       "94  1687964058482484               1.50       2.00         1.50           1.5   \n",
       "95  1685651809260972               3.50       3.50         2.50           2.0   \n",
       "96  1662420268098487               3.00       4.00         3.00           3.0   \n",
       "97  1687976114519479               3.50       4.50         4.00           3.0   \n",
       "98  1687976114519971               2.00       2.50         2.00           1.5   \n",
       "\n",
       "    grammar_avg  cefr_avg                              question_1_transcript  \\\n",
       "0          4.50       4.5  So my favorite TV show would be the, uh the Fi...   \n",
       "1          3.67       4.0  Yeah, I would be happy to go to the past and m...   \n",
       "2          5.00       5.0  I would much rather be a first version me than...   \n",
       "3          4.00       4.0  My advice to a worker in my office is if you d...   \n",
       "4          2.50       2.5  I'm working in the production, in my, in, in m...   \n",
       "..          ...       ...                                                ...   \n",
       "94         1.50       1.5  My favorite class in school is informatics. Er...   \n",
       "95         2.50       2.5  I use, uh, uh, computer because it, it's so fu...   \n",
       "96         3.00       3.0  the person who is the most important people. N...   \n",
       "97         3.00       4.0  I will travel with my parents to Colombia beca...   \n",
       "98         1.50       1.5  A life in Panama. Um, um, my house is big and ...   \n",
       "\n",
       "                                question_2_transcript  \\\n",
       "0   Uh, there are actually many different jobs I d...   \n",
       "1   I prefer to be most remembered for my great wi...   \n",
       "2   This is an excellent question. I think I would...   \n",
       "3   Uh, I would, I would love to communicate with ...   \n",
       "4   I'm, uh, got to talk in the teacher of the, an...   \n",
       "..                                                ...   \n",
       "94  My family is, uh, make me happy. Um, my family...   \n",
       "95  We are six people and my father is uh is a war...   \n",
       "96  my best teacher was maybe, I don't know. My my...   \n",
       "97  I think the biggest problem for kids today is ...   \n",
       "98  I'm going to talk about, of my mom and my mom ...   \n",
       "\n",
       "                                question_3_transcript  \\\n",
       "0   Uh, I consider charity organizations important...   \n",
       "1   I prefer to be the first person to explore a p...   \n",
       "2   This is an interesting question. The potential...   \n",
       "3   That's a good question. I do not have, I don't...   \n",
       "4   Er, I like to go to the, er, in engineering, i...   \n",
       "..                                                ...   \n",
       "94  In the last, in the vacation of last year I tr...   \n",
       "95  I'm gonna talk about my friend, my best friend...   \n",
       "96                                                NaN   \n",
       "97  I didn't know about the system of recycling in...   \n",
       "98  And my favorite vacation is when I go to Cancu...   \n",
       "\n",
       "                                question_4_transcript  \\\n",
       "0   I, I think that the home schooling becomes uh ...   \n",
       "1   I think there should be restrictions for publi...   \n",
       "2   I think that this is an excellent question. It...   \n",
       "3   Ok. Then if I'll have an, if I have an ideal p...   \n",
       "4   No, I just, uh, stay just me. I, I like the, w...   \n",
       "..                                                ...   \n",
       "94  In my time happy when my sister, not, not the ...   \n",
       "95  And for me, uh, an interesting, some, somethin...   \n",
       "96  Well, I think um different um work office is n...   \n",
       "97  If I start my own business, it will be a cafet...   \n",
       "98  Ok. My, my friend is, um, his wife is a small ...   \n",
       "\n",
       "                                question_5_transcript  \\\n",
       "0   I consider studying foreign languages. Uh real...   \n",
       "1   Yeah, I prefer to be remembered for my great w...   \n",
       "2   As a student for an All girls high school, I a...   \n",
       "3   In my city. Uh the system is that uh everyone ...   \n",
       "4   No, I don't want to move to a new city or, uh,...   \n",
       "..                                                ...   \n",
       "94  I don't think, er, planning, er, to last summe...   \n",
       "95  To the next summer I will go to, to the don't ...   \n",
       "96  is is um, difficult question because, um, I th...   \n",
       "97  I think high school students will have to wear...   \n",
       "98  Ok. In my next summer I will play, er, I love ...   \n",
       "\n",
       "                                         transcript-1  \\\n",
       "0   So my favorite TV show would be The Firefly. I...   \n",
       "1   I would be happy to go to the past and meet my...   \n",
       "2   I would much rather be a first version me than...   \n",
       "3   My advice to a worker in my office is if you d...   \n",
       "4   I am working in the production in my job, I am...   \n",
       "..                                                ...   \n",
       "94  My favorite class in school is informatics bec...   \n",
       "95  I use a computer because it's so fun to use it...   \n",
       "96  The person who is the most important person in...   \n",
       "97  I will travel with my parents to Colombia beca...   \n",
       "98  My life in Panama, my house is big and I can d...   \n",
       "\n",
       "                                         transcript-2  \\\n",
       "0   There are actually many different jobs I don't...   \n",
       "1   I prefer to be most remembered for my great wi...   \n",
       "2   This is an excellent question. I think I would...   \n",
       "3   I would love to communicate with dolphins. I d...   \n",
       "4   I am going to talk to the teacher of the Engli...   \n",
       "..                                                ...   \n",
       "94  My family is make me happy. My family is more....   \n",
       "95  We are six people and my father is a work, is ...   \n",
       "96  My best teacher was maybe, I don't know, my fi...   \n",
       "97  I think the biggest problem for kids today is ...   \n",
       "98  I'm going to talk about my mom. My mom is impo...   \n",
       "\n",
       "                                         transcript-3  \\\n",
       "0   I consider charity organizations important bec...   \n",
       "1   I prefer to be the first person to explore a p...   \n",
       "2   This is an interesting question. The potential...   \n",
       "3   That's a good question. I do not have... I don...   \n",
       "4   I like to go to the engineering in my workplac...   \n",
       "..                                                ...   \n",
       "94  In the last, in the vacation of last year, I t...   \n",
       "95  I'm gonna talk about my friend, my best friend...   \n",
       "96  Thank you. Bye bye. Bye bye. Bye bye. Bye bye....   \n",
       "97  I didn't know about the system of recycling in...   \n",
       "98  My favorite vacation is when I go to Cancun be...   \n",
       "\n",
       "                                         transcript-4  \\\n",
       "0   I think that homeschooling became really popul...   \n",
       "1   I think there should be restrictions for publi...   \n",
       "2   I think that this is an excellent question. It...   \n",
       "3   Okay then, if I have an ideal project, I would...   \n",
       "4   No, I just stay just me. I like what I do, I l...   \n",
       "..                                                ...   \n",
       "94  I'm happy when my sister not destroy the house...   \n",
       "95  And for me, an interesting, something interest...   \n",
       "96  Well, I think different work office is not usu...   \n",
       "97  If I start my own business it will be a cafete...   \n",
       "98  Okay, my friend is white, he is a small person...   \n",
       "\n",
       "                                         transcript-5  \n",
       "0   I consider studying foreign languages really i...  \n",
       "1   Yeah, I prefer to be remembered for my great w...  \n",
       "2   As a student for an all-girls high school, I a...  \n",
       "3   In my city, the system is that everyone should...  \n",
       "4   No, I don't want to move to a new city or to a...  \n",
       "..                                                ...  \n",
       "94  I don't think planning to next summer. Where w...  \n",
       "95  to the next summer i will go to the down cost ...  \n",
       "96  It's a difficult question because I think the ...  \n",
       "97  I think high school students should have to we...  \n",
       "98  Okay, my next summer I will play a lot of Play...  \n",
       "\n",
       "[99 rows x 17 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "341f94f9-d5dc-45ab-848d-2feb7ec77df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfirstindexvalue(text):\n",
    "    try:\n",
    "        retval = text.split('_')[1]\n",
    "    except:\n",
    "        retval = \" \"\n",
    "    return retval\n",
    "    \n",
    "def get_transcript_df(get_transcript_df, provided):\n",
    "    df['assessment_id'] = df['assessment_id'].astype(str)\n",
    "    \n",
    "    # Reshape the DataFrame\n",
    "    if provided == True:\n",
    "        reshaped_df = pd.melt(df[['assessment_id', 'question_1_transcript','question_2_transcript','question_3_transcript','question_4_transcript','question_5_transcript']], id_vars=['assessment_id'], var_name='value', value_name='transcript')\n",
    "        reshaped_df['assessment_id'] = reshaped_df['assessment_id'] + '-' + reshaped_df['value'].apply(getfirstindexvalue)\n",
    "    else:\n",
    "        reshaped_df = pd.melt(df[['assessment_id', 'transcript-1','transcript-2','transcript-3','transcript-4','transcript-5']], id_vars=['assessment_id'], var_name='value', value_name='transcript')\n",
    "        reshaped_df['assessment_id'] = reshaped_df['assessment_id'] + '-' + reshaped_df['value'].str.split('-').str[-1]        \n",
    "\n",
    "    # # Drop the 'value' column\n",
    "    reshaped_df.drop(columns=['value'], inplace=True)\n",
    "    \n",
    "    return reshaped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d775a2cb-2b90-46cb-b058-27198f8a293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brunets_index(transcript):\n",
    "    \"\"\"\n",
    "    Calculate Brunet's Index for lexical diversity of a text.\n",
    "\n",
    "    Args:\n",
    "    - text: List of words representing the text.\n",
    "\n",
    "    Returns:\n",
    "    - Brunet's Index value.\n",
    "    \"\"\"\n",
    "    if not transcript is None: \n",
    "        transcript = str(transcript)\n",
    "        text_without_punctuation = transcript.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "        \n",
    "        transcript_list = text_without_punctuation.split()\n",
    "        \n",
    "        # Calculate the total number of words in the text\n",
    "        total_words = len(transcript)\n",
    "    \n",
    "        # Calculate the total number of unique words in the text\n",
    "        unique_words = len(set(transcript))\n",
    "    \n",
    "        # Calculate Brunet's Index\n",
    "        brunets_index = unique_words ** (0.165)\n",
    "    else:\n",
    "        brunets_index = None\n",
    "        \n",
    "    return brunets_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d12cbe-ebad-4ea5-b1d9-1442977995d8",
   "metadata": {},
   "source": [
    "##### Compute the average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb8a6990-1298-48a2-9c98-3adacfa12459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text into sentences and words, and compute average sentence length\n",
    "def average_sentence_length(transcript):\n",
    "\n",
    "    if not transcript is None:\n",
    "        # Tokenize text into sentences\n",
    "        sentences = nltk.sent_tokenize(str(transcript))\n",
    "    \n",
    "        # Tokenize each sentence into words\n",
    "        tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "        # Calculate total number of words and sentences\n",
    "        total_words = sum(len(sentence) for sentence in tokenized_sentences)\n",
    "        total_sentences = len(tokenized_sentences)\n",
    "    \n",
    "        # Calculate average sentence length\n",
    "        if total_sentences == 0:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "    return total_words / total_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f2a730-d3ff-41fc-8179-05c2f3a80c6a",
   "metadata": {},
   "source": [
    "##### Compute text cohesion\n",
    "This refers to the degree of semantic relatedness or connectedness between words in a text. It is a measure of how well the words in a text are integrated or linked together in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0163bf1-6237-4e99-bff3-16f68476cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cohesion based on cosine similarity of word embeddings\n",
    "def embedding_cohesion(text):\n",
    "\n",
    "    if not text is None:\n",
    "        text = str(text).lower()\n",
    "        # Tokenize text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Calculate pairwise cosine similarity between word embeddings\n",
    "        similarities = []\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i+1, len(words)):\n",
    "                word1 = words[i]\n",
    "                word2 = words[j]\n",
    "                if word1 in embeddings and word2 in embeddings:\n",
    "                    similarity = cosine_similarity([embeddings[word1]], [embeddings[word2]])[0][0]\n",
    "                    similarities.append(similarity)\n",
    "        \n",
    "        # Calculate average similarity (cohesion)\n",
    "        if similarities:\n",
    "            return np.mean(similarities)\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48e26c-9640-40c2-a5f6-73ee8c01529c",
   "metadata": {},
   "source": [
    "##### Compute the flesch_kincaid grade level and coleman_liau index of the transcript, to be added to the transcript data\n",
    "Both are readability measures:\n",
    "<br>- Flesch-Kincaid Grade Level: takes into account syllable count and sentence length\n",
    "<br>- Coleman-Liau Index: primarily considers word length and sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563e300f-99b8-48b0-846c-893594c68494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count syllables in a word\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    if len(word) <= 3:\n",
    "        return 1\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] in vowels and word[i - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    return count\n",
    "    \n",
    "# Function to compute Flesch-Kincaid Grade Level\n",
    "def flesch_kincaid_grade_level(transcript):\n",
    "    if transcript is None:\n",
    "        return 0\n",
    "    else:\n",
    "        transcript = str(transcript)\n",
    "        sentences = sent_tokenize(transcript)\n",
    "        words = word_tokenize(transcript)\n",
    "        num_sentences = len(sentences)\n",
    "        num_words = len(words)\n",
    "        num_syllables = sum([syllable_count(word) for word in words])\n",
    "        \n",
    "        # Compute Flesch-Kincaid Grade Level\n",
    "        if num_sentences == 0 or num_words == 0:\n",
    "            return 0\n",
    "        return 0.39 * (num_words / num_sentences) + 11.8 * (num_syllables / num_words) - 15.59\n",
    "\n",
    "# Function to compute Coleman-Liau Index\n",
    "def coleman_liau_index(transcript):\n",
    "\n",
    "    if transcript is None:\n",
    "        return 0\n",
    "    else:\n",
    "        transcript = str(transcript)\n",
    "        sentences = sent_tokenize(transcript)\n",
    "        words = word_tokenize(transcript)\n",
    "        num_sentences = len(sentences)\n",
    "        num_words = len(words)\n",
    "        \n",
    "        if len(words) == 0:\n",
    "            return 0\n",
    "        \n",
    "        num_characters = sum(len(word) for word in words)\n",
    "        \n",
    "        # Compute Coleman-Liau Index\n",
    "        L = num_characters / num_words * 100\n",
    "        S = num_sentences / num_words * 100\n",
    "        return 0.0588 * L - 0.296 * S - 15.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956b6fa-da62-4341-a32b-eccf2ca7dbc4",
   "metadata": {},
   "source": [
    "#### Generate the Automated Readability Index (ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cb838db-074b-4ab9-8888-3f4f675df339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_readability_index(transcript):\n",
    "    \"\"\"\n",
    "    Calculate the Automated Readability Index (ARI) of a text.\n",
    "    \n",
    "    Parameters:\n",
    "        transcript (text): the text of the transcript\n",
    "        \n",
    "    Returns:\n",
    "        float: The Automated Readability Index (ARI) score.\n",
    "    \"\"\"\n",
    "    transcript = str(transcript)\n",
    "    \n",
    "    characters = len(transcript)\n",
    "    words = len(transcript.split())\n",
    "    sentences = transcript.count('.') + transcript.count('!') + transcript.count('?')  # Assuming sentences end with '.', '!', or '?'\n",
    "\n",
    "    if len(transcript)>0 and sentences == 0:\n",
    "        ari = 1\n",
    "    elif words > 0 and sentences > 0:\n",
    "        ari = 4.71 * (characters / words) + 0.5 * (words / sentences) - 21.43\n",
    "    else: \n",
    "        ari = 0\n",
    "    \n",
    "    return ari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a7326-247b-4ded-a395-69a4bd377cbc",
   "metadata": {},
   "source": [
    "##### Count repeated words in the transcript, to be added to the transcript data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49691833-db9d-4259-948b-9b03113de69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect repetitions in text\n",
    "def detect_repetitions(transcript):\n",
    "    # Define regular expression pattern to match repeated sequences of words\n",
    "    pattern = re.compile(r'\\b(\\w+)\\s+\\1\\b', re.IGNORECASE)\n",
    "\n",
    "    transcript = str(transcript)\n",
    "    \n",
    "    if transcript is None:\n",
    "        return 0\n",
    "    else:\n",
    "        # Find all repeated sequences in the text\n",
    "        repetitions = re.findall(pattern, transcript)\n",
    "    \n",
    "        return len(repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9e077-6a2f-411f-b88e-1e4245fd62ee",
   "metadata": {},
   "source": [
    "##### Count filler words in the transcript, to be added to the transcript data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69d32a5d-af49-40f4-8d39-21229925e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count filler words in the transcript\n",
    "def filler_words(transcript):\n",
    "    filler_words = [\"um\", \"uh\", \"like\", \"you know\", \"actually\", \"basically\", \"hmm\", \"ahem\", \"i mean\", \"so\"]  \n",
    "    if not transcript is None:\n",
    "        transcript = str(transcript)\n",
    "        cnt = sum(str(transcript).lower().count(word) for word in filler_words)\n",
    "    else:\n",
    "        cnt = 0\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e13bc2-d340-43b2-8caf-c50320693976",
   "metadata": {},
   "source": [
    "##### Merge all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ab09b1-24d0-4f86-9646-40fa9f4ceaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(df, transcript_df, features_df):\n",
    "    df['assessment_id'] = df['assessment_id'].astype(str)\n",
    "\n",
    "    # Duplicate the rows for each assessment_id and rename the assessment_id's with -1 to -5 suffixes accordingly\n",
    "    temp = df[['assessment_id', 'pronunciation_avg', 'vocab_avg', 'fluency_avg', 'cohesion_avg', 'grammar_avg', 'cefr_avg']]\n",
    "    df_duplicated = pd.concat([temp]*5, ignore_index=True)\n",
    "    \n",
    "    suffixes = ['-1', '-2', '-3', '-4', '-5']\n",
    "    df_duplicated['suffix'] = df_duplicated.groupby(['assessment_id']).cumcount() % len(suffixes)\n",
    "    df_duplicated['assessment_id'] = df_duplicated['assessment_id'] + '-' + df_duplicated['suffix'].add(1).astype(str)\n",
    "    \n",
    "    df_duplicated = df_duplicated.sort_values(by=['assessment_id']).reset_index(drop=True)\n",
    "    \n",
    "    df_full = df_duplicated.merge(transcript_df, left_on='assessment_id', right_on='assessment_id', how='inner')\n",
    "    df_full = df_full.merge(features_df, left_on='assessment_id', right_on='audio_file', how='inner')\n",
    "    df_full.drop(columns=['audio_file','audio_duration','sampling_rate','channels'], inplace=True)\n",
    "    df_full = df_full.sort_values(by='assessment_id').reset_index(drop=True)\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c2308-5138-4670-8aec-159153930f1d",
   "metadata": {},
   "source": [
    "##### Prepare full dataset for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773e8e5-95d9-4c61-9e33-030f39534d6d",
   "metadata": {},
   "source": [
    "##### Run all code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a708dce9-250b-4df6-ab94-0677ca204acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running automated_readability_index\n",
      ">>> Running detect_repetitions()\n",
      ">>> Writing the dataframe to file\n",
      ">>> Running read_transcript()\n",
      ">>> Running get_transcript_df() - extracting the provided transcripts\n",
      ">>> Running filler_words()\n",
      ">>> Running merge_datasets()\n",
      ">>> Running get_brunets_index()\n",
      ">>> Running average_sentence_length()\n",
      ">>> Running embedding_cohesion()\n",
      ">>> Running flesch_kincaid_grade_level()\n",
      ">>> Running coleman_liau_index()\n",
      ">>> Running automated_readability_index\n",
      ">>> Running detect_repetitions()\n",
      ">>> Writing the dataframe to file\n",
      ">>> Execution complete\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Execution Initiated\")\n",
    "\n",
    "# print(\">>> Running retrieve_audio_features()\")\n",
    "\n",
    "# Generate the initial audio feature set \n",
    "features_df = retrieve_audio_features(mp3_files)\n",
    "\n",
    "print(\">>> Loading the provided transcripts\")\n",
    "\n",
    "# Load the CSV file and add the transcripts\n",
    "df = pd.read_csv('./Data/text/SpeakNow_test_data.csv')\n",
    "\n",
    "\n",
    "#### First extract the provided transcripts ####\n",
    "\n",
    "# Get and reshape the transcript data from the dataset  \n",
    "transcripts_df = get_transcript_df(df, provided=True)\n",
    "\n",
    "print(\">>> Running filler_words()\")\n",
    "\n",
    "# Add the filler words to the data\n",
    "transcripts_df['filler_word_count'] = transcripts_df.transcript.apply(filler_words)\n",
    "\n",
    "print(\">>> Running merge_datasets()\")\n",
    "\n",
    "# Merge the datasets\n",
    "full_text_df = merge_datasets(df, transcripts_df, features_df)\n",
    "\n",
    "print(\">>> Running get_brunets_index()\")\n",
    "\n",
    "# Get Brunets index (measure of lexical diversity that indicates the richness of the vocabulary used)\n",
    "full_text_df['brunets_index'] = full_text_df.apply(lambda row: get_brunets_index(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running average_sentence_length()\")\n",
    "\n",
    "full_text_df['average_sentence_length'] = full_text_df.apply(lambda row: average_sentence_length(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running embedding_cohesion()\")\n",
    "\n",
    "full_text_df['cohesion_score'] = full_text_df.apply(lambda row: embedding_cohesion(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running flesch_kincaid_grade_level()\")\n",
    "\n",
    "full_text_df['flesch_kincaid_score'] = full_text_df.apply(lambda row: flesch_kincaid_grade_level(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running coleman_liau_index()\")\n",
    "\n",
    "full_text_df['coleman_liau_index'] = full_text_df.apply(lambda row: coleman_liau_index(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running automated_readability_index\")\n",
    "\n",
    "full_text_df['automated_readability_index'] = full_text_df.apply(lambda row: automated_readability_index(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running detect_repetitions()\")\n",
    "\n",
    "full_text_df['repetitions'] = full_text_df.apply(lambda row: detect_repetitions(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Writing the dataframe to file\")\n",
    "\n",
    "full_text_df.to_csv(\"full_df_provided_transcripts.csv\", index=False)\n",
    "\n",
    "### Next extract the transcripts as transcribed from the mp3 files ####\n",
    "\n",
    "# Get and reshape the transcript data from the dataset\n",
    "print(\">>> Running read_transcript()\")\n",
    "\n",
    "# Read the transcripts\n",
    "for i in range(1,6):\n",
    "    df[f'transcript-{str(i)}'] = df.apply(lambda row: read_transcript(chatgpt_transcripts, row['assessment_id'], str(i)), axis=1)\n",
    "\n",
    "print(\">>> Running get_transcript_df() - extracting the provided transcripts\")\n",
    "transcripts_df = get_transcript_df(df, provided=False)\n",
    "\n",
    "print(\">>> Running filler_words()\")\n",
    "\n",
    "# Add the filler words to the data\n",
    "transcripts_df['filler_word_count'] = transcripts_df.transcript.apply(filler_words)\n",
    "\n",
    "print(\">>> Running merge_datasets()\")\n",
    "\n",
    "# Merge the datasets\n",
    "full_speech_df = merge_datasets(df, transcripts_df, features_df)\n",
    "\n",
    "print(\">>> Running get_brunets_index()\")\n",
    "\n",
    "# Get Brunets index (measure of lexical diversity that indicates the richness of the vocabulary used)\n",
    "full_speech_df['brunets_index'] = full_speech_df.apply(lambda row: get_brunets_index(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running average_sentence_length()\")\n",
    "\n",
    "full_speech_df['average_sentence_length'] = full_speech_df.apply(lambda row: average_sentence_length(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running embedding_cohesion()\")\n",
    "\n",
    "full_speech_df['cohesion_score'] = full_speech_df.apply(lambda row: embedding_cohesion(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running flesch_kincaid_grade_level()\")\n",
    "\n",
    "full_speech_df['flesch_kincaid_score'] = full_speech_df.apply(lambda row: flesch_kincaid_grade_level(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running coleman_liau_index()\")\n",
    "\n",
    "full_speech_df['coleman_liau_index'] = full_speech_df.apply(lambda row: coleman_liau_index(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running automated_readability_index\")\n",
    "\n",
    "full_speech_df['automated_readability_index'] = full_speech_df.apply(lambda row: automated_readability_index(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Running detect_repetitions()\")\n",
    "\n",
    "full_speech_df['repetitions'] = full_speech_df.apply(lambda row: detect_repetitions(row['transcript']), axis=1)\n",
    "\n",
    "print(\">>> Writing the dataframe to file\")\n",
    "\n",
    "full_speech_df.to_csv(\"full_df_transcribed_transcripts.csv\", index=False)\n",
    "\n",
    "print(\">>> Execution complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "843a6ac8-7a82-401f-b3c4-a433fd4f6b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 495 entries, 0 to 494\n",
      "Data columns (total 33 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   assessment_id                   495 non-null    object \n",
      " 1   pronunciation_avg               495 non-null    float64\n",
      " 2   vocab_avg                       495 non-null    float64\n",
      " 3   fluency_avg                     495 non-null    float64\n",
      " 4   cohesion_avg                    495 non-null    float64\n",
      " 5   grammar_avg                     495 non-null    float64\n",
      " 6   cefr_avg                        495 non-null    float64\n",
      " 7   suffix                          495 non-null    int64  \n",
      " 8   transcript                      328 non-null    object \n",
      " 9   filler_word_count               495 non-null    int64  \n",
      " 10  formants_mean                   495 non-null    float64\n",
      " 11  formants_median                 495 non-null    float64\n",
      " 12  formants_std                    495 non-null    float64\n",
      " 13  formants_min                    495 non-null    float64\n",
      " 14  formants_max                    495 non-null    float64\n",
      " 15  mfccs_mean                      495 non-null    float32\n",
      " 16  mfccs_std                       495 non-null    float32\n",
      " 17  mfccs_var                       495 non-null    float32\n",
      " 18  spectral_centroid_mean          495 non-null    float64\n",
      " 19  spectral_bandwidth_mean         495 non-null    float64\n",
      " 20  pitch                           495 non-null    float32\n",
      " 21  intensity                       495 non-null    float32\n",
      " 22  speech_rate                     495 non-null    float64\n",
      " 23  intensity_to_speech_rate_ratio  495 non-null    float64\n",
      " 24  pauses_duration                 495 non-null    float64\n",
      " 25  pauses_frequency                495 non-null    float64\n",
      " 26  brunets_index                   495 non-null    float64\n",
      " 27  average_sentence_length         495 non-null    float64\n",
      " 28  cohesion_score                  495 non-null    float64\n",
      " 29  flesch_kincaid_score            495 non-null    float64\n",
      " 30  coleman_liau_index              495 non-null    float64\n",
      " 31  automated_readability_index     495 non-null    float64\n",
      " 32  repetitions                     495 non-null    int64  \n",
      "dtypes: float32(5), float64(23), int64(3), object(2)\n",
      "memory usage: 118.1+ KB\n"
     ]
    }
   ],
   "source": [
    "full_text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7557d3c-fdc7-4f49-83cb-8092c94d60dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assessment_id</th>\n",
       "      <th>pronunciation_avg</th>\n",
       "      <th>vocab_avg</th>\n",
       "      <th>fluency_avg</th>\n",
       "      <th>cohesion_avg</th>\n",
       "      <th>grammar_avg</th>\n",
       "      <th>cefr_avg</th>\n",
       "      <th>suffix</th>\n",
       "      <th>transcript</th>\n",
       "      <th>filler_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>intensity_to_speech_rate_ratio</th>\n",
       "      <th>pauses_duration</th>\n",
       "      <th>pauses_frequency</th>\n",
       "      <th>brunets_index</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>cohesion_score</th>\n",
       "      <th>flesch_kincaid_score</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>repetitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1647885061811312-1</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>I would be happy to go to the past and meet my...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.386965</td>\n",
       "      <td>1.771535</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>0.625115</td>\n",
       "      <td>7.790643</td>\n",
       "      <td>3.826071</td>\n",
       "      <td>12.208367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1647885061811312-2</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I prefer to be most remembered for my great wi...</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414006</td>\n",
       "      <td>0.059125</td>\n",
       "      <td>0.377043</td>\n",
       "      <td>1.762279</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>0.580784</td>\n",
       "      <td>10.124844</td>\n",
       "      <td>8.358165</td>\n",
       "      <td>15.898763</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1647885061811312-3</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>I prefer to be the first person to explore a p...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437442</td>\n",
       "      <td>0.050563</td>\n",
       "      <td>0.348857</td>\n",
       "      <td>1.762279</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>0.569591</td>\n",
       "      <td>5.872581</td>\n",
       "      <td>3.689784</td>\n",
       "      <td>9.678438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1647885061811312-4</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>I think there should be restrictions for publi...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337909</td>\n",
       "      <td>0.057375</td>\n",
       "      <td>0.363708</td>\n",
       "      <td>1.752770</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.603732</td>\n",
       "      <td>12.707931</td>\n",
       "      <td>9.081724</td>\n",
       "      <td>19.104533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1647885061811312-5</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Yeah, I prefer to be remembered for my great w...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425983</td>\n",
       "      <td>0.050812</td>\n",
       "      <td>0.415432</td>\n",
       "      <td>1.762279</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>0.604109</td>\n",
       "      <td>10.329906</td>\n",
       "      <td>6.048302</td>\n",
       "      <td>16.031154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1693181003292353-1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>I live in Taipei and I like this area because ...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726157</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.456051</td>\n",
       "      <td>1.722563</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>0.578910</td>\n",
       "      <td>7.201538</td>\n",
       "      <td>2.916308</td>\n",
       "      <td>10.402204</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>1693181003292353-2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>I think doctors are the most important job now...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542273</td>\n",
       "      <td>0.043625</td>\n",
       "      <td>0.416716</td>\n",
       "      <td>1.732930</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>0.622965</td>\n",
       "      <td>7.497310</td>\n",
       "      <td>4.881519</td>\n",
       "      <td>11.708750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>1693181003292353-3</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>I think in Taiwan the internet maybe is not ex...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598825</td>\n",
       "      <td>0.049625</td>\n",
       "      <td>0.408646</td>\n",
       "      <td>1.797924</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.560056</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>6.240476</td>\n",
       "      <td>16.359831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>1693181003292353-4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "      <td>I don't like to study online because I cannot ...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620194</td>\n",
       "      <td>0.045562</td>\n",
       "      <td>0.443971</td>\n",
       "      <td>1.752770</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.615540</td>\n",
       "      <td>6.122703</td>\n",
       "      <td>4.207568</td>\n",
       "      <td>9.293177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1693181003292353-5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4</td>\n",
       "      <td>I think students have peace is important becau...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599247</td>\n",
       "      <td>0.032188</td>\n",
       "      <td>0.421096</td>\n",
       "      <td>1.732930</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>0.619343</td>\n",
       "      <td>3.999808</td>\n",
       "      <td>3.456000</td>\n",
       "      <td>9.175678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          assessment_id  pronunciation_avg  vocab_avg  fluency_avg  \\\n",
       "0    1647885061811312-1               3.33       4.33         3.67   \n",
       "1    1647885061811312-2               3.33       4.33         3.67   \n",
       "2    1647885061811312-3               3.33       4.33         3.67   \n",
       "3    1647885061811312-4               3.33       4.33         3.67   \n",
       "4    1647885061811312-5               3.33       4.33         3.67   \n",
       "..                  ...                ...        ...          ...   \n",
       "490  1693181003292353-1               3.00       3.50         2.50   \n",
       "491  1693181003292353-2               3.00       3.50         2.50   \n",
       "492  1693181003292353-3               3.00       3.50         2.50   \n",
       "493  1693181003292353-4               3.00       3.50         2.50   \n",
       "494  1693181003292353-5               3.00       3.50         2.50   \n",
       "\n",
       "     cohesion_avg  grammar_avg  cefr_avg  suffix  \\\n",
       "0             4.0         3.67       4.0       0   \n",
       "1             4.0         3.67       4.0       1   \n",
       "2             4.0         3.67       4.0       2   \n",
       "3             4.0         3.67       4.0       3   \n",
       "4             4.0         3.67       4.0       4   \n",
       "..            ...          ...       ...     ...   \n",
       "490           2.5         2.50       2.5       0   \n",
       "491           2.5         2.50       2.5       1   \n",
       "492           2.5         2.50       2.5       2   \n",
       "493           2.5         2.50       2.5       3   \n",
       "494           2.5         2.50       2.5       4   \n",
       "\n",
       "                                            transcript  filler_word_count  \\\n",
       "0    I would be happy to go to the past and meet my...                  2   \n",
       "1    I prefer to be most remembered for my great wi...                  6   \n",
       "2    I prefer to be the first person to explore a p...                  4   \n",
       "3    I think there should be restrictions for publi...                  5   \n",
       "4    Yeah, I prefer to be remembered for my great w...                  1   \n",
       "..                                                 ...                ...   \n",
       "490  I live in Taipei and I like this area because ...                  4   \n",
       "491  I think doctors are the most important job now...                  2   \n",
       "492  I think in Taiwan the internet maybe is not ex...                  3   \n",
       "493  I don't like to study online because I cannot ...                  3   \n",
       "494  I think students have peace is important becau...                  1   \n",
       "\n",
       "     ...  intensity_to_speech_rate_ratio  pauses_duration  pauses_frequency  \\\n",
       "0    ...                        0.377500         0.047500          0.386965   \n",
       "1    ...                        0.414006         0.059125          0.377043   \n",
       "2    ...                        0.437442         0.050563          0.348857   \n",
       "3    ...                        0.337909         0.057375          0.363708   \n",
       "4    ...                        0.425983         0.050812          0.415432   \n",
       "..   ...                             ...              ...               ...   \n",
       "490  ...                        0.726157         0.044750          0.456051   \n",
       "491  ...                        0.542273         0.043625          0.416716   \n",
       "492  ...                        0.598825         0.049625          0.408646   \n",
       "493  ...                        0.620194         0.045562          0.443971   \n",
       "494  ...                        0.599247         0.032188          0.421096   \n",
       "\n",
       "     brunets_index  average_sentence_length  cohesion_score  \\\n",
       "0         1.771535                22.400000        0.625115   \n",
       "1         1.762279                21.800000        0.580784   \n",
       "2         1.762279                17.375000        0.569591   \n",
       "3         1.752770                29.000000        0.603732   \n",
       "4         1.762279                26.500000        0.604109   \n",
       "..             ...                      ...             ...   \n",
       "490       1.722563                21.666667        0.578910   \n",
       "491       1.732930                19.750000        0.622965   \n",
       "492       1.797924                21.000000        0.560056   \n",
       "493       1.752770                12.333333        0.615540   \n",
       "494       1.732930                16.250000        0.619343   \n",
       "\n",
       "     flesch_kincaid_score  coleman_liau_index  automated_readability_index  \\\n",
       "0                7.790643            3.826071                    12.208367   \n",
       "1               10.124844            8.358165                    15.898763   \n",
       "2                5.872581            3.689784                     9.678438   \n",
       "3               12.707931            9.081724                    19.104533   \n",
       "4               10.329906            6.048302                    16.031154   \n",
       "..                    ...                 ...                          ...   \n",
       "490              7.201538            2.916308                    10.402204   \n",
       "491              7.497310            4.881519                    11.708750   \n",
       "492              8.333333            6.240476                    16.359831   \n",
       "493              6.122703            4.207568                     9.293177   \n",
       "494              3.999808            3.456000                     9.175678   \n",
       "\n",
       "     repetitions  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "..           ...  \n",
       "490            0  \n",
       "491            0  \n",
       "492            0  \n",
       "493            0  \n",
       "494            0  \n",
       "\n",
       "[495 rows x 33 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_speech_df[~full_speech_df['transcript'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "524a3f31-d1a5-4541-9e26-f18d4d311b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_ax_features = full_speech_df[~full_speech_df['transcript'].isna()][['pronunciation_avg',\n",
    "                              'vocab_avg',\n",
    "                              'fluency_avg',\n",
    "                              'cohesion_avg',\n",
    "                              'grammar_avg',\n",
    "                              'cefr_avg',\n",
    "                              'filler_word_count',\n",
    "                              'formants_mean',\n",
    "                              'formants_median',\n",
    "                              'formants_std',\n",
    "                              'formants_min',\n",
    "                              'formants_max',\n",
    "                              'mfccs_mean',\n",
    "                              'mfccs_std',\n",
    "                              'mfccs_var',\n",
    "                              'spectral_centroid_mean',\n",
    "                              'spectral_bandwidth_mean',\n",
    "                              'pitch',\n",
    "                              'intensity',\n",
    "                              'speech_rate',\n",
    "                              'intensity_to_speech_rate_ratio',\n",
    "                              'pauses_duration',\n",
    "                              'pauses_frequency',\n",
    "                              'average_sentence_length',\n",
    "                              'cohesion_score',\n",
    "                              'flesch_kincaid_score',\n",
    "                              'coleman_liau_index',\n",
    "                              'automated_readability_index',                                        \n",
    "                              'repetitions',\n",
    "                              'brunets_index']]\n",
    "\n",
    "writing_ax_features = full_text_df[~full_text_df['transcript'].isna()][['pronunciation_avg',\n",
    "                               'vocab_avg',\n",
    "                               'fluency_avg',\n",
    "                               'cohesion_avg',\n",
    "                               'grammar_avg',\n",
    "                               'cefr_avg',\n",
    "                               'average_sentence_length',\n",
    "                               'cohesion_score',\n",
    "                               'flesch_kincaid_score',\n",
    "                               'coleman_liau_index',\n",
    "                               'automated_readability_index',                                        \n",
    "                               'repetitions',\n",
    "                               'brunets_index']]\n",
    "\n",
    "all_features = full_df[~full_df['transcript'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0e27302-9c20-40f1-8d8b-9972b2e7aa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronunciation_avg</th>\n",
       "      <th>vocab_avg</th>\n",
       "      <th>fluency_avg</th>\n",
       "      <th>cohesion_avg</th>\n",
       "      <th>grammar_avg</th>\n",
       "      <th>cefr_avg</th>\n",
       "      <th>filler_word_count</th>\n",
       "      <th>formants_mean</th>\n",
       "      <th>formants_median</th>\n",
       "      <th>formants_std</th>\n",
       "      <th>...</th>\n",
       "      <th>intensity_to_speech_rate_ratio</th>\n",
       "      <th>pauses_duration</th>\n",
       "      <th>pauses_frequency</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>cohesion_score</th>\n",
       "      <th>flesch_kincaid_score</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>repetitions</th>\n",
       "      <th>brunets_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1575.008259</td>\n",
       "      <td>1280.371703</td>\n",
       "      <td>1077.019747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.386965</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>0.625115</td>\n",
       "      <td>7.790643</td>\n",
       "      <td>3.826071</td>\n",
       "      <td>12.208367</td>\n",
       "      <td>0</td>\n",
       "      <td>1.771535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1485.328376</td>\n",
       "      <td>1233.364249</td>\n",
       "      <td>1026.021623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414006</td>\n",
       "      <td>0.059125</td>\n",
       "      <td>0.377043</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>0.580784</td>\n",
       "      <td>10.124844</td>\n",
       "      <td>8.358165</td>\n",
       "      <td>15.898763</td>\n",
       "      <td>0</td>\n",
       "      <td>1.762279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1577.452085</td>\n",
       "      <td>1300.776096</td>\n",
       "      <td>1015.432021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437442</td>\n",
       "      <td>0.050563</td>\n",
       "      <td>0.348857</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>0.569591</td>\n",
       "      <td>5.872581</td>\n",
       "      <td>3.689784</td>\n",
       "      <td>9.678438</td>\n",
       "      <td>0</td>\n",
       "      <td>1.762279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1694.951272</td>\n",
       "      <td>1442.866943</td>\n",
       "      <td>1013.853015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337909</td>\n",
       "      <td>0.057375</td>\n",
       "      <td>0.363708</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.603732</td>\n",
       "      <td>12.707931</td>\n",
       "      <td>9.081724</td>\n",
       "      <td>19.104533</td>\n",
       "      <td>0</td>\n",
       "      <td>1.752770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1694.526295</td>\n",
       "      <td>1396.984445</td>\n",
       "      <td>1132.584517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425983</td>\n",
       "      <td>0.050812</td>\n",
       "      <td>0.415432</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>0.604109</td>\n",
       "      <td>10.329906</td>\n",
       "      <td>6.048302</td>\n",
       "      <td>16.031154</td>\n",
       "      <td>0</td>\n",
       "      <td>1.762279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1905.328966</td>\n",
       "      <td>1838.152440</td>\n",
       "      <td>1015.867877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726157</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.456051</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>0.578910</td>\n",
       "      <td>7.201538</td>\n",
       "      <td>2.916308</td>\n",
       "      <td>10.402204</td>\n",
       "      <td>0</td>\n",
       "      <td>1.722563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1970.857456</td>\n",
       "      <td>1920.705853</td>\n",
       "      <td>1077.252573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542273</td>\n",
       "      <td>0.043625</td>\n",
       "      <td>0.416716</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>0.622965</td>\n",
       "      <td>7.497310</td>\n",
       "      <td>4.881519</td>\n",
       "      <td>11.708750</td>\n",
       "      <td>0</td>\n",
       "      <td>1.732930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1820.651146</td>\n",
       "      <td>1492.729742</td>\n",
       "      <td>1060.049836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598825</td>\n",
       "      <td>0.049625</td>\n",
       "      <td>0.408646</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.560056</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>6.240476</td>\n",
       "      <td>16.359831</td>\n",
       "      <td>0</td>\n",
       "      <td>1.797924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1976.972462</td>\n",
       "      <td>1832.188552</td>\n",
       "      <td>1058.423466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620194</td>\n",
       "      <td>0.045562</td>\n",
       "      <td>0.443971</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.615540</td>\n",
       "      <td>6.122703</td>\n",
       "      <td>4.207568</td>\n",
       "      <td>9.293177</td>\n",
       "      <td>0</td>\n",
       "      <td>1.752770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1877.179862</td>\n",
       "      <td>1604.744285</td>\n",
       "      <td>1121.745944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599247</td>\n",
       "      <td>0.032188</td>\n",
       "      <td>0.421096</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>0.619343</td>\n",
       "      <td>3.999808</td>\n",
       "      <td>3.456000</td>\n",
       "      <td>9.175678</td>\n",
       "      <td>0</td>\n",
       "      <td>1.732930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pronunciation_avg  vocab_avg  fluency_avg  cohesion_avg  grammar_avg  \\\n",
       "0                 3.33       4.33         3.67           4.0         3.67   \n",
       "1                 3.33       4.33         3.67           4.0         3.67   \n",
       "2                 3.33       4.33         3.67           4.0         3.67   \n",
       "3                 3.33       4.33         3.67           4.0         3.67   \n",
       "4                 3.33       4.33         3.67           4.0         3.67   \n",
       "..                 ...        ...          ...           ...          ...   \n",
       "490               3.00       3.50         2.50           2.5         2.50   \n",
       "491               3.00       3.50         2.50           2.5         2.50   \n",
       "492               3.00       3.50         2.50           2.5         2.50   \n",
       "493               3.00       3.50         2.50           2.5         2.50   \n",
       "494               3.00       3.50         2.50           2.5         2.50   \n",
       "\n",
       "     cefr_avg  filler_word_count  formants_mean  formants_median  \\\n",
       "0         4.0                  2    1575.008259      1280.371703   \n",
       "1         4.0                  6    1485.328376      1233.364249   \n",
       "2         4.0                  4    1577.452085      1300.776096   \n",
       "3         4.0                  5    1694.951272      1442.866943   \n",
       "4         4.0                  1    1694.526295      1396.984445   \n",
       "..        ...                ...            ...              ...   \n",
       "490       2.5                  4    1905.328966      1838.152440   \n",
       "491       2.5                  2    1970.857456      1920.705853   \n",
       "492       2.5                  3    1820.651146      1492.729742   \n",
       "493       2.5                  3    1976.972462      1832.188552   \n",
       "494       2.5                  1    1877.179862      1604.744285   \n",
       "\n",
       "     formants_std  ...  intensity_to_speech_rate_ratio  pauses_duration  \\\n",
       "0     1077.019747  ...                        0.377500         0.047500   \n",
       "1     1026.021623  ...                        0.414006         0.059125   \n",
       "2     1015.432021  ...                        0.437442         0.050563   \n",
       "3     1013.853015  ...                        0.337909         0.057375   \n",
       "4     1132.584517  ...                        0.425983         0.050812   \n",
       "..            ...  ...                             ...              ...   \n",
       "490   1015.867877  ...                        0.726157         0.044750   \n",
       "491   1077.252573  ...                        0.542273         0.043625   \n",
       "492   1060.049836  ...                        0.598825         0.049625   \n",
       "493   1058.423466  ...                        0.620194         0.045562   \n",
       "494   1121.745944  ...                        0.599247         0.032188   \n",
       "\n",
       "     pauses_frequency  average_sentence_length  cohesion_score  \\\n",
       "0            0.386965                22.400000        0.625115   \n",
       "1            0.377043                21.800000        0.580784   \n",
       "2            0.348857                17.375000        0.569591   \n",
       "3            0.363708                29.000000        0.603732   \n",
       "4            0.415432                26.500000        0.604109   \n",
       "..                ...                      ...             ...   \n",
       "490          0.456051                21.666667        0.578910   \n",
       "491          0.416716                19.750000        0.622965   \n",
       "492          0.408646                21.000000        0.560056   \n",
       "493          0.443971                12.333333        0.615540   \n",
       "494          0.421096                16.250000        0.619343   \n",
       "\n",
       "     flesch_kincaid_score  coleman_liau_index  automated_readability_index  \\\n",
       "0                7.790643            3.826071                    12.208367   \n",
       "1               10.124844            8.358165                    15.898763   \n",
       "2                5.872581            3.689784                     9.678438   \n",
       "3               12.707931            9.081724                    19.104533   \n",
       "4               10.329906            6.048302                    16.031154   \n",
       "..                    ...                 ...                          ...   \n",
       "490              7.201538            2.916308                    10.402204   \n",
       "491              7.497310            4.881519                    11.708750   \n",
       "492              8.333333            6.240476                    16.359831   \n",
       "493              6.122703            4.207568                     9.293177   \n",
       "494              3.999808            3.456000                     9.175678   \n",
       "\n",
       "     repetitions  brunets_index  \n",
       "0              0       1.771535  \n",
       "1              0       1.762279  \n",
       "2              0       1.762279  \n",
       "3              0       1.752770  \n",
       "4              0       1.762279  \n",
       "..           ...            ...  \n",
       "490            0       1.722563  \n",
       "491            0       1.732930  \n",
       "492            0       1.797924  \n",
       "493            0       1.752770  \n",
       "494            0       1.732930  \n",
       "\n",
       "[495 rows x 30 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_ax_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
